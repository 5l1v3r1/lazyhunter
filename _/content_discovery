# {{ CONTENT DISCOVERY WORKFLOW

echo -e "[${blue}+${reset}] content discovery"

# {{ CONTENT DISCOVERY WORKFLOW: FINGERPRINT

[ ${fingerprint} == True ] && [ -f ${hosts} ] && {
    echo -e "    [${blue}+${reset}] fingerprinting"
    fingerprinting_output="${content_discovery_output}/technology"

    [ ! -d ${fingerprinting_output} ] && mkdir -p ${fingerprinting_output}

    printf "        [${blue}+${reset}] wafw00f"
    printf "\r"
    waf_fingerprinting_output="${fingerprinting_output}/waf.json"
    wafw00f -i ${hosts} -o ${waf_fingerprinting_output} &> /dev/null
    echo -e "        [${green}*${reset}] wafw00f"

    # echo -e "        [${blue}+${reset}] web application technology"
    # web_fingerprinting_output="${fingerprinting_output}/web-technology"
    # [ ! -d ${web_fingerprinting_output} ] && mkdir -p ${web_fingerprinting_output}
    # cat ${hosts} | rush 'wappalyzer {} -P > {output_dir}/$(echo {} | urlbits format %s.%S.%r.%t).json' -j 5 -v output_dir=${web_fingerprinting_output}
}

# }}
# {{ CONTENT DISCOVERY WORKFLOW: CRAWL

[ ${crawl} == True ] && {
    echo -e "    [${blue}+${reset}] crawl"

    [ ${domain} != False ] && {
        printf "        [${blue}+${reset}] sigurls"
        printf "\r"
        [ ${include_subdomains} == True ] && {
            sigurls -d ${domain} -iS -silent 1> ${urls} 2> /dev/null
        } || {
            sigurls -d ${domain} -silent 1> ${urls} 2> /dev/null
        }
        echo -e "        [${green}*${reset}] sigurls"
    }

    echo -e "        [${blue}+${reset}] sigrawler"
    local sigrawler_output="${content_discovery_output}/temp-sigrawler-crawl.json"

    [ -f ${hosts} ] && {
        [ ${include_subdomains} == True ] && {
             cat ${hosts} ${urls} | sigrawler -iL - -depth 3 -iS -oJ ${sigrawler_output} &> /dev/null
        } || {
             cat ${hosts} ${urls} | sigrawler -iL - -depth 3 -oJ ${sigrawler_output} &> /dev/null
        }
    } || {
        [ ${include_subdomains} == True ] && {
            cat ${urls} | sigrawler -iL - -depth 3 -iS -oJ ${sigrawler_output} &> /dev/null
        } || {
            cat ${urls} | sigrawler -iL - -depth 3 -oJ ${sigrawler_output} &> /dev/null
        }
    }

    echo -e "            [${green}+${reset}] extract:"

    jq -r '.s3[]?' ${sigrawler_output} | anew -q ${s3_buckets}
    echo -e "                [${green}+${reset}] s3: $(jq -r '.s3[]?' ${sigrawler_output} | sort -u | wc -l)"

    jq -r '.urls[]?' ${sigrawler_output} | anew -q ${urls}
    echo -e "                [${green}+${reset}] urls: $(jq -r '.urls[]?' ${sigrawler_output} | sort -u | wc -l)"

    [ ${keep} == False ] && rm ${content_discovery_output}/temp-*-crawl.json
}

# }}
# {{ CONTENT DISCOVERY WORKFLOW: PROBE URLS

[ ${urls_probing} == True ] && [ -f ${urls} ] && {
    echo -e "    [${blue}+${reset}] probe urls"

    printf "        [${blue}+${reset}] categorize urls"
    printf "\r"
    sigurlx -iL ${urls} -c -oJ ${urls_categories} &> /dev/null
    echo -e "        [${green}*${reset}] categorize urls"

    printf "        [${blue}+${reset}] probe endpoints"
    printf "\r"
    jq -r '.[] | select(.category == "endpoint") | .url' ${urls_categories} | sigurlx -iL - -pV -pR -dX -r -oJ ${endpoints_probe} &> /dev/null
    echo -e "        [${green}*${reset}] probe endpoints"
}

# }}

# }}